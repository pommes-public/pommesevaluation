{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164a145a",
   "metadata": {},
   "source": [
    "# Converter from pommesinvest to AMIRIS\n",
    "Convert pommesinvest results to AMIRIS input data format\n",
    "\n",
    "> **IMPORTANT**\n",
    ">\n",
    "> * Run `investment_results_inspection.ipynb` once before.\n",
    "> * Run `exogenous_plant_analyses.ipynb` once before.\n",
    "> * Ensure necessary model input resp. results data is available at the respective locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4bc3ce",
   "metadata": {},
   "source": [
    "## Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ad1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from pommesevaluation.amiris_converter import (\n",
    "    convert_annual_data_to_fame_time, \n",
    "    convert_time_series_index_to_fame_time,\n",
    "    resample_to_hourly_frequency,\n",
    "    extract_net_operation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305e498",
   "metadata": {},
   "source": [
    "## Notebook and workflow settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "time_frame_in_years = 26\n",
    "freq = \"1H\"\n",
    "multiplier = {\n",
    "    \"1H\": 1,\n",
    "    \"4H\": 4,\n",
    "    \"8H\": 8,\n",
    "}\n",
    "fuel_cost_pathway = \"NZE\"\n",
    "fuel_price_shock = \"high\"\n",
    "emissions_cost_pathway = \"long-term\"\n",
    "\n",
    "# Paths to data needed to be converted\n",
    "path_model_inputs = \"./model_inputs/pommesinvest/\"\n",
    "path_model_results = \"./model_results/pommesinvest/\"\n",
    "path_processed_outputs = \"./data_out/\"\n",
    "\n",
    "# Filename and scenarios\n",
    "filename_investment = \"results_for_amiris_total_dr_scenario\"\n",
    "filename_backup = \"installed_capacity_backup_generation.csv\"\n",
    "filename_storage = \"installed_storage_capacity.csv\"\n",
    "file_name_res_capacities = \"sources_renewables_investment_model.csv\"\n",
    "file_name_res_generation = \"sources_renewables_ts_hourly.csv\"\n",
    "\n",
    "# Map corresponding flex options scenario (values) to dr scenarios (keys)\n",
    "dr_scenarios = {\n",
    "    \"none\": \"50\",\n",
    "    \"5\": \"95\", \n",
    "    \"50\": \"50\",\n",
    "    \"95\": \"5\"\n",
    "}\n",
    "\n",
    "file_names_demand = {\n",
    "    scen: f\"sinks_demand_el_excl_demand_response_{scen}.csv\"\n",
    "    for scen in dr_scenarios\n",
    "}\n",
    "file_names_demand_ts = {\n",
    "    scen: f\"sinks_demand_el_excl_demand_response_ts_{scen}_hourly.csv\"\n",
    "    for scen in dr_scenarios\n",
    "}\n",
    "file_names_demand[\"none\"] = \"sinks_demand_el.csv\"\n",
    "file_names_demand_ts[\"none\"] = \"sinks_demand_el_ts_hourly.csv\"\n",
    "\n",
    "file_names_dispatch_results = {\n",
    "    scen: (\n",
    "        f\"investment_LP_start-2020-01-01_{time_frame_in_years}\"\n",
    "        f\"-years_simple_freq_{freq}_with_dr_{scen}_\"\n",
    "        f\"fuel_price-{fuel_cost_pathway}_{fuel_price_shock}_\"\n",
    "        f\"co2_price-{emissions_cost_pathway}_production.csv\"\n",
    "    ) for scen in dr_scenarios\n",
    "}\n",
    "file_names_dispatch_results[\"none\"] = (\n",
    "    f\"investment_LP_start-2020-01-01_{time_frame_in_years}\"\n",
    "    f\"-years_simple_freq_{freq}_no_dr_50_\"\n",
    "    f\"fuel_price-{fuel_cost_pathway}_{fuel_price_shock}_\"\n",
    "    f\"co2_price-{emissions_cost_pathway}_production.csv\"\n",
    ")\n",
    "file_name_demand_response_eligibility = \"demand_response_clusters_eligibility.csv\"\n",
    "\n",
    "file_name_co2_prices = f\"costs_emissions_{emissions_cost_pathway}_nominal_indexed_ts.csv\"\n",
    "file_name_fuel_prices = f\"costs_fuel_{fuel_cost_pathway}_{fuel_price_shock}_nominal_indexed_ts.csv\"\n",
    "file_names_opex = {\n",
    "    scen: f\"variable_costs_{flex_scenario}%_nominal.csv\"\n",
    "    for scen, flex_scenario in dr_scenarios.items()\n",
    "}\n",
    "file_names_fixed_costs = {\n",
    "    scen: f\"fixed_costs_{flex_scenario}%_nominal.csv\"\n",
    "    for scen, flex_scenario in dr_scenarios.items()\n",
    "}\n",
    "file_names_investment_expenses = {\n",
    "    scen: f\"investment_expenses_{flex_scenario}%_nominal.csv\"\n",
    "    for scen, flex_scenario in dr_scenarios.items()\n",
    "}\n",
    "\n",
    "file_name_investment_options = \"transformers_investment_options.csv\"\n",
    "file_name_availabilities = \"transformers_availability_ts_hourly.csv\"\n",
    "\n",
    "file_name_transformers = \"transformers_exogenous.csv\"\n",
    "file_name_transformers_max = \"transformers_exogenous_max_ts.csv\"\n",
    "\n",
    "# Define which pieces of information to exclude from investments\n",
    "exclude_from_investment = [\n",
    "    \"PHS_inflow\", \n",
    "    \"battery_inflow\",\n",
    "]\n",
    "\n",
    "# Define demand response cluster to focus on in AMIRIS analyses\n",
    "demand_response_focus_cluster = \"ind_cluster_shift_only\"\n",
    "\n",
    "file_names_baseline_load = {\n",
    "    scen: f\"{path_model_inputs}sinks_demand_response_el_ts_{scen}.csv\"\n",
    "    for scen in dr_scenarios\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39750b86",
   "metadata": {},
   "source": [
    "# Read in and convert\n",
    "## Obtain demand response clusters before actual results extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_response_clusters = pd.read_csv(f\"{path_model_inputs}{file_name_demand_response_eligibility}\", index_col=0)\n",
    "demand_response_cluster_index = demand_response_clusters.index\n",
    "exclude_from_investment.extend(demand_response_cluster_index)\n",
    "exclude_from_investment.remove(demand_response_focus_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c72be2",
   "metadata": {},
   "source": [
    "## Transformers & renewables\n",
    "* Different for all scenarios: Investment results\n",
    "    * transformers + demand response focus cluster only\n",
    "* Same across all scenarios:\n",
    "    * Backup generation (exogenous transformers)\n",
    "    * Renewable capacities & generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investments per scenario\n",
    "for dr_scenario in dr_scenarios:\n",
    "    invest_results_all = pd.read_csv(\n",
    "        f\"{path_processed_outputs}{filename_investment}_{dr_scenario}.csv\", \n",
    "        index_col=0, header=[0, 1]\n",
    "    )\n",
    "    invest_results = invest_results_all[[\n",
    "        col for col in invest_results_all.columns if col[0] not in exclude_from_investment\n",
    "    ]]\n",
    "    _ = convert_annual_data_to_fame_time(\n",
    "        invest_results, \n",
    "        save=True, \n",
    "        path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "        filename=\"installed_capacity_ts\",\n",
    "        rounding_precision=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f689f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup and RES\n",
    "backup_capacities = pd.read_csv(f\"{path_processed_outputs}{filename_backup}\", index_col=0)\n",
    "res_generation = pd.read_csv(f\"{path_model_inputs}{file_name_res_generation}\", index_col=0)\n",
    "\n",
    "_ = convert_annual_data_to_fame_time(\n",
    "    backup_capacities,\n",
    "    save=True,\n",
    "    path=f\"{path_processed_outputs}/amiris/all_scenarios/\",\n",
    "    filename=\"exogenous_installed_capacity_ts\",\n",
    "    rounding_precision=2,\n",
    ")\n",
    "_ = convert_time_series_index_to_fame_time(\n",
    "    res_generation,\n",
    "    save=True,\n",
    "    path=f\"{path_processed_outputs}/amiris/all_scenarios/\",\n",
    "    filename=\"res_generation_ts\",\n",
    "    rounding_precision=4,\n",
    ")\n",
    "\n",
    "res_capacities_2020 = pd.read_csv(f\"{path_model_inputs}{file_name_res_capacities}\", index_col=0)\n",
    "res_capacities_2020 = res_capacities_2020.loc[res_capacities_2020[\"country\"] == \"DE\"]\n",
    "res_capacities_2020.index = res_capacities_2020.index.str.split(\"_\", expand=True).get_level_values(2)\n",
    "res_capacities_2020 = res_capacities_2020[\"capacity\"].round(2)\n",
    "res_capacities_2020.to_csv(f\"{path_processed_outputs}/amiris/all_scenarios/installed_renewable_capacities_2020.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ea0f2",
   "metadata": {},
   "source": [
    "## \"Net\" Demand\n",
    "Since in AMIRIS, there is a shortcoming in terms of modelling competing demand-side flexibility options, all flexibility options except the one demand response-cluster that is focused upon are not modelled explicitly, but their dispatch is considered in calculating a net demand, i.e. a \n",
    "* demand after\n",
    "    * imports and exports, (add net exports, i.e. additional demand)\n",
    "    * storages, (subtract net outflow)\n",
    "    * demand response (except for the focus cluster)\n",
    "    * electrolyzer operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_demand = {}\n",
    "demand_after_flexibility = {}\n",
    "\n",
    "for dr_scenario in dr_scenarios:\n",
    "    # Original demand\n",
    "    sinks_demand_el = pd.read_csv(f\"{path_model_inputs}{file_names_demand[dr_scenario]}\", index_col=0)\n",
    "    max_demand = sinks_demand_el.at[\"DE_sink_el_load\", \"maximum\"]\n",
    "    sinks_demand_el_ts = pd.read_csv(f\"{path_model_inputs}{file_names_demand_ts[dr_scenario]}\", index_col=0)\n",
    "    original_demand[dr_scenario] = sinks_demand_el_ts[\"DE_sink_el_load\"] * max_demand\n",
    "    \n",
    "    demand_after_flex = original_demand[dr_scenario].copy()\n",
    "    demand_after_flex.index = pd.to_datetime(demand_after_flex.index)\n",
    "    \n",
    "    # Read in dispatch results\n",
    "    dispatch_results = pd.read_csv(\n",
    "        f\"{path_model_results}{file_names_dispatch_results[dr_scenario]}\", index_col=0\n",
    "    )\n",
    "    \n",
    "    # Filter imports and exports and add net exports (exports - imports) to demand\n",
    "    net_exports = extract_net_operation(\n",
    "        dispatch_results,\n",
    "        column_str=\"link_\",\n",
    "        outflow_column_str=\"DE_link_\",\n",
    "        inflow_column_str=\"_link_DE\",\n",
    "    )\n",
    "    \n",
    "    # Resample (different frequency) and adjust demand after flex\n",
    "    net_exports = resample_to_hourly_frequency(net_exports, multiplier[freq])    \n",
    "    demand_after_flex += net_exports\n",
    "    \n",
    "    # Filter electrical storage results and subtract net storage (outflow - inflow) from demand\n",
    "    net_storage_outflow = extract_net_operation(\n",
    "        dispatch_results,\n",
    "        column_str=\"DE_storage_el\",\n",
    "        outflow_column_str=\", 'DE_bus_el')\",\n",
    "        inflow_column_str=\"('DE_bus_el',\",\n",
    "    )\n",
    "    \n",
    "    # Resample (different frequency) and adjust demand after flex\n",
    "    net_storage_outflow = resample_to_hourly_frequency(net_storage_outflow, multiplier[freq])    \n",
    "    demand_after_flex -= net_storage_outflow\n",
    "    \n",
    "    # Filter demand response results except for the focus cluster and add net demand\n",
    "    demand_response_results = dispatch_results[\n",
    "        [\n",
    "            col for col in dispatch_results.columns\n",
    "            for cluster in demand_response_clusters.index\n",
    "            if cluster in col and \", 'flow')\" in col\n",
    "            and cluster != demand_response_focus_cluster\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Drop duplicates\n",
    "    demand_response_results = demand_response_results.loc[:, ~demand_response_results.columns.duplicated()]\n",
    "    try:\n",
    "        demand_response_results = resample_to_hourly_frequency(demand_response_results, multiplier[freq])\n",
    "        demand_after_flex += demand_response_results.sum(axis=1)\n",
    "    # Skip in case no demand response is modelled\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Filter electrolyzer dispatch and add electrolyzer electricity consumption to demand\n",
    "    electrolyzer_dispatch = extract_net_operation(\n",
    "        dispatch_results,\n",
    "        column_str=\"electrolyzer\",\n",
    "        outflow_column_str=\"NO_OUTFLOW\",  # outflow goes to hydrogen bus\n",
    "        inflow_column_str=\"('DE_bus_el',\",\n",
    "    ).mul(-1)\n",
    "    \n",
    "    # Resample (different frequency) and adjust demand after flex\n",
    "    electrolyzer_dispatch = resample_to_hourly_frequency(electrolyzer_dispatch, multiplier[freq])    \n",
    "    demand_after_flex += electrolyzer_dispatch\n",
    "    \n",
    "    # Increase load by baseline demand of load shifting focus cluster\n",
    "    if dr_scenario != \"none\":\n",
    "        max_capacity_focus_cluster = pd.read_csv((\n",
    "                f\"{path_model_inputs}{demand_response_focus_cluster}_\"\n",
    "                f\"potential_parameters_{dr_scenario}%.csv\"\n",
    "            ), \n",
    "            index_col = 0\n",
    "        )[[\"max_cap\"]]\n",
    "        max_capacity_focus_cluster.index = max_capacity_focus_cluster.index.astype(str) + \"-01-01 00:00:00\"\n",
    "        max_capacity_focus_cluster = resample_to_hourly_frequency(max_capacity_focus_cluster, multiplier[freq])\n",
    "        max_capacity_focus_cluster.index = max_capacity_focus_cluster.index.astype(str)\n",
    "        \n",
    "        baseline_load_profile_focus_cluster = pd.read_csv(\n",
    "            file_names_baseline_load[dr_scenario], \n",
    "            index_col=0\n",
    "        )[demand_response_focus_cluster]\n",
    "        baseline_load_profile_focus_cluster = (\n",
    "            baseline_load_profile_focus_cluster * max_capacity_focus_cluster[\"max_cap\"]\n",
    "        )\n",
    "        \n",
    "        demand_after_flex += baseline_load_profile_focus_cluster.values\n",
    "        \n",
    "    # Store demand after flexibility; adjust to FAME time and save file\n",
    "    demand_after_flexibility[dr_scenario] = demand_after_flex.round(2)\n",
    "\n",
    "    _ = convert_time_series_index_to_fame_time(\n",
    "        demand_after_flex,\n",
    "        save=True,\n",
    "        path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "        filename=\"demand_after_flex_ts\",\n",
    "        rounding_precision=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a08b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(demand_after_flexibility), figsize=(15, 5 * len(demand_after_flexibility)))\n",
    "\n",
    "for ax, (key, value) in enumerate(demand_after_flexibility.items()):\n",
    "    value.plot(ax=axs[ax])\n",
    "    axs[ax].set_ylim(0, 200000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec27ee4",
   "metadata": {},
   "source": [
    "## Prices and costs\n",
    "Extract prices and costs from input data and convert to FAME format:\n",
    "* Same across all scenarios:\n",
    "    * CO2 price\n",
    "    * Fuel prices\n",
    "* Different per scenario:\n",
    "    * OPEX\n",
    "    * fixed costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc289b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_price = pd.read_csv(f\"{path_model_inputs}{file_name_co2_prices}\", index_col=0)\n",
    "co2_price.index = co2_price.index.str[:4]\n",
    "co2_price = co2_price[[\"DE_source_hardcoal\"]]\n",
    "co2_price.columns = [\"nominal_value\"]\n",
    "\n",
    "fuel_prices = pd.read_csv(f\"{path_model_inputs}{file_name_fuel_prices}\", index_col=0)\n",
    "fuel_prices.index = fuel_prices.index.str[:4]\n",
    "fuel_prices = fuel_prices[[col for col in fuel_prices.columns if \"DE_\" in col]]\n",
    "\n",
    "_ = convert_annual_data_to_fame_time(\n",
    "    co2_price,\n",
    "    save=True,\n",
    "    path=f\"{path_processed_outputs}/amiris/all_scenarios/\",\n",
    "    filename=\"emissions_costs\",\n",
    "    rounding_precision=3,\n",
    ")\n",
    "_ = convert_annual_data_to_fame_time(\n",
    "    fuel_prices,\n",
    "    save=True,\n",
    "    path=f\"{path_processed_outputs}/amiris/all_scenarios/\",\n",
    "    filename=\"fuel_prices\",\n",
    "    rounding_precision=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEX, fixed costs and specific investment expenses per scenario\n",
    "for dr_scenario in dr_scenarios:\n",
    "    opex = pd.read_csv(f\"{path_model_inputs}{file_names_opex[dr_scenario]}\", index_col=0)\n",
    "    opex.index = opex.index.str[:4]\n",
    "    _ = convert_annual_data_to_fame_time(\n",
    "        opex, \n",
    "        save=True, \n",
    "        path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "        filename=\"opex\",\n",
    "        rounding_precision=3,\n",
    "    )\n",
    "\n",
    "    # Fixed costs are given as percentage of investment per year; thus need to be calculated first if used at all\n",
    "    fixed_costs = pd.read_csv(f\"{path_model_inputs}{file_names_fixed_costs[dr_scenario]}\", index_col=0)\n",
    "    fixed_costs.to_csv(f\"{path_processed_outputs}/amiris/{dr_scenario}/fixed_costs.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6205ec",
   "metadata": {},
   "source": [
    "## Availabilities for conventionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "availabilities = pd.read_csv(f\"{path_model_inputs}{file_name_availabilities}\", index_col=0)\n",
    "_ = convert_time_series_index_to_fame_time(\n",
    "    availabilities,\n",
    "    save=True,\n",
    "    path=f\"{path_processed_outputs}/amiris/all_scenarios/\",\n",
    "    filename=\"availabilities\",\n",
    "    rounding_precision=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3315ed",
   "metadata": {},
   "source": [
    "## Demand response\n",
    "Extract the following pieces of information:\n",
    "* Load shedding by eligible clusters:\n",
    "    * overall availability time series for load shedding\n",
    "    * variable costs for shedding\n",
    "* Demand response focus cluster (shifting):\n",
    "    * normalized availability for upshift resp. downshift of focus cluster\n",
    "    * costs and potential parameters for demand response focus cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281db421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare hourly frequency investment results for creating shedding time series\n",
    "hourly_invest_results_all = invest_results_all.copy()\n",
    "hourly_invest_results_all.loc[2051] = hourly_invest_results_all.iloc[-1]\n",
    "hourly_invest_results_all.index = pd.to_datetime([f\"{idx}-01-01\" for idx in hourly_invest_results_all.index])\n",
    "hourly_invest_results_all = hourly_invest_results_all.resample(\"H\").ffill()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dr_scenario in dr_scenarios:\n",
    "    if dr_scenario != \"none\":\n",
    "        availability_pos_ts = pd.read_csv(\n",
    "            f\"{path_model_inputs}sinks_demand_response_el_ava_pos_ts_{dr_scenario}.csv\", \n",
    "            index_col=0\n",
    "        )\n",
    "        availability_pos_ts.index = pd.to_datetime(availability_pos_ts.index)\n",
    "        availability_neg_ts = pd.read_csv(\n",
    "            f\"{path_model_inputs}sinks_demand_response_el_ava_neg_ts_{dr_scenario}.csv\", \n",
    "            index_col=0\n",
    "        )\n",
    "        baseline_load_profile = pd.read_csv(\n",
    "            f\"{path_model_inputs}sinks_demand_response_el_ts_{dr_scenario}.csv\", \n",
    "            index_col=0\n",
    "        )\n",
    "\n",
    "        # Prepare data to parameterize load shedding\n",
    "        for dr_cluster in demand_response_clusters.loc[demand_response_clusters[\"shedding\"] == 1].index:\n",
    "            try:\n",
    "                var_costs = pd.read_csv(\n",
    "                    f\"{path_model_inputs}{dr_cluster}_variable_costs_parameters_{dr_scenario}%.csv\", \n",
    "                    index_col=0\n",
    "                )\n",
    "                var_costs.index = var_costs.index.str[:4]\n",
    "                var_costs.loc[\"2020\"].to_csv(\n",
    "                    f\"{path_processed_outputs}/amiris/{dr_scenario}/\"\n",
    "                    f\"{dr_cluster}_variable_costs_2020.csv\"\n",
    "                )\n",
    "                _ = convert_annual_data_to_fame_time(\n",
    "                    var_costs,\n",
    "                    save=True,\n",
    "                    path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "                    filename=dr_cluster,\n",
    "                    rounding_precision=3,\n",
    "                )\n",
    "                availability_pos_ts[dr_cluster] *= hourly_invest_results_all[(dr_cluster, dr_cluster)]\n",
    "                _ = convert_time_series_index_to_fame_time(\n",
    "                    availability_pos_ts[dr_cluster],\n",
    "                    save=True,\n",
    "                    path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "                    filename=f\"availability_shedding_{dr_cluster}\",\n",
    "                    rounding_precision=3,\n",
    "                )\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "        # Prepare data for demand response focus cluster\n",
    "\n",
    "        # Costs\n",
    "        var_costs = pd.read_csv(\n",
    "            f\"{path_model_inputs}{demand_response_focus_cluster}_variable_costs_parameters_{dr_scenario}%.csv\", \n",
    "            index_col=0\n",
    "        )\n",
    "        var_costs.index = var_costs.index.str[:4]\n",
    "        # Extract costs for 2020 separately since AMIRIS can only handle scalars\n",
    "        var_costs.loc[\"2020\"].to_csv(\n",
    "            f\"{path_processed_outputs}/amiris/{dr_scenario}/\"\n",
    "            f\"{demand_response_focus_cluster}_variable_costs_2020.csv\",\n",
    "        )\n",
    "        _ = convert_annual_data_to_fame_time(\n",
    "            var_costs,\n",
    "            save=True,\n",
    "            path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "            filename=demand_response_focus_cluster,\n",
    "            rounding_precision=3,\n",
    "        )\n",
    "        fixed_costs_and_investments = pd.read_csv((\n",
    "                f\"{path_model_inputs}{demand_response_focus_cluster}_\"\n",
    "                f\"fixed_costs_and_investments_parameters_{dr_scenario}%.csv\"\n",
    "            ), \n",
    "            index_col=0\n",
    "        )\n",
    "        fixed_costs_and_investments.index = fixed_costs_and_investments.index.str[:4]\n",
    "        _ = convert_annual_data_to_fame_time(\n",
    "            fixed_costs_and_investments,\n",
    "            save=True,\n",
    "            path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "            filename=demand_response_focus_cluster,\n",
    "            rounding_precision=3,\n",
    "        )\n",
    "        \n",
    "        # Potential parameters\n",
    "        potential_parameters = pd.read_csv((\n",
    "                f\"{path_model_inputs}{demand_response_focus_cluster}_\"\n",
    "                f\"potential_parameters_{dr_scenario}%.csv\"\n",
    "            ), \n",
    "            index_col = 0\n",
    "        )\n",
    "        # Extract parameters for 2020 separately since AMIRIS can only handle scalars for some values (mapping)\n",
    "        potential_parameters.loc[2020].to_csv(\n",
    "            f\"{path_processed_outputs}/amiris/{dr_scenario}/\"\n",
    "            f\"{demand_response_focus_cluster}_potential_parameters_2020.csv\"\n",
    "        )\n",
    "        _ = convert_annual_data_to_fame_time(\n",
    "            potential_parameters,\n",
    "            save=True,\n",
    "            path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "            filename=demand_response_focus_cluster,\n",
    "            rounding_precision=3,\n",
    "        )\n",
    "\n",
    "        # Baseline load time series and availabilities\n",
    "        _ = convert_time_series_index_to_fame_time(\n",
    "            baseline_load_profile[demand_response_focus_cluster],\n",
    "            save=True,\n",
    "            path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "            filename=f\"baseline_load_profile_{demand_response_focus_cluster}\",\n",
    "            rounding_precision=4,\n",
    "        )\n",
    "        _ = convert_time_series_index_to_fame_time(\n",
    "            availability_pos_ts[demand_response_focus_cluster],\n",
    "            save=True,\n",
    "            path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "            filename=f\"availability_shifting_pos_{demand_response_focus_cluster}\",\n",
    "            rounding_precision=4,\n",
    "        )\n",
    "        _ = convert_time_series_index_to_fame_time(\n",
    "            availability_neg_ts[demand_response_focus_cluster],\n",
    "            save=True,\n",
    "            path=f\"{path_processed_outputs}/amiris/{dr_scenario}/\",\n",
    "            filename=f\"availability_shifting_neg_{demand_response_focus_cluster}\",\n",
    "            rounding_precision=4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5f285",
   "metadata": {},
   "source": [
    "# Derive efficiencies for conventionals\n",
    "## Existing / exogenous plants\n",
    "For existing conventional plants, use a regression approach to estimate minimum and maximum efficiency values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in transformers (conventionals) data\n",
    "transformers = pd.read_csv(f\"{path_model_inputs}{file_name_transformers}\", index_col=0)\n",
    "transformers_max_ts = pd.read_csv(f\"{path_model_inputs}{file_name_transformers_max}\", index_col=0)\n",
    "\n",
    "# Combine information to derive installed capacities by year\n",
    "transformers_capacity_ts = transformers_max_ts.mul(transformers[\"capacity\"])\n",
    "\n",
    "# Group by tech_fuel\n",
    "transformers_capacity_ts.index = transformers_capacity_ts.index.str[:4]\n",
    "transformers_capacity_ts_transposed = transformers_capacity_ts.T\n",
    "transformers_capacity_ts_transposed[[\"efficiency_el\", \"tech_fuel\"]] = (\n",
    "    transformers[[\"efficiency_el\", \"tech_fuel\"]]\n",
    ")\n",
    "grouped_plants = {\n",
    "    tech_fuel: plants.sort_values(by=\"efficiency_el\") \n",
    "     for tech_fuel, plants in transformers_capacity_ts_transposed.groupby(\"tech_fuel\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ab116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a regression analysis to derive efficiencies and calculate installed capacities meanwhile\n",
    "for key, value in grouped_plants.items():\n",
    "    power_plants_regression = pd.DataFrame(\n",
    "        index=range(2020, 2051), \n",
    "        columns=[\"efficiency_min\", \"efficiency_max\", \"installed_cap\"]\n",
    "    )\n",
    "    for iter_year in range(2020, 2051):\n",
    "        value[\"cumulated_capacity\"] = value[str(iter_year)].cumsum()\n",
    "\n",
    "        X = grouped_plants[key].cumulated_capacity.values\n",
    "        Y = grouped_plants[key].efficiency_el.values\n",
    "\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        efficiency_regression = sm.OLS(Y, X).fit()\n",
    "\n",
    "        x = np.linspace(0, X.max(), int(X.max()))\n",
    "        if len(Y) > 1:\n",
    "            regression_function = efficiency_regression.params[0] + efficiency_regression.params[1] * x\n",
    "            min_efficiency = max(0.1, round(regression_function[0], 4))\n",
    "            max_efficiency = round(regression_function[-1], 4)\n",
    "        # Only one entry in group; thus no regression function\n",
    "        else:\n",
    "            min_efficiency = round(Y[0], 4)\n",
    "            max_efficiency = round(Y[0], 4)\n",
    "\n",
    "        power_plants_regression.at[iter_year, \"efficiency_min\"] = min_efficiency\n",
    "        power_plants_regression.at[iter_year, \"efficiency_max\"] = max_efficiency\n",
    "        power_plants_regression.at[iter_year, \"exogenous_installed_cap\"] = value[\"cumulated_capacity\"].iloc[-1]\n",
    "\n",
    "    _ = convert_annual_data_to_fame_time(\n",
    "        power_plants_regression,\n",
    "        save=True,\n",
    "        path=f\"{path_processed_outputs}/amiris/all_scenarios/\",\n",
    "        filename=f\"{key}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6196a",
   "metadata": {},
   "source": [
    "## Endogenous installations\n",
    "Extract single efficiency value from input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6799434",
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_options = pd.read_csv(f\"{path_model_inputs}/{file_name_investment_options}\", index_col=0)\n",
    "\n",
    "investment_options[\"new_index\"] = investment_options.index.str.split(\n",
    "    \"_\", 2, expand=True\n",
    ").get_level_values(2).str.rsplit(\n",
    "    \"_\", 2, expand=True\n",
    ").get_level_values(0)\n",
    "\n",
    "investment_options = investment_options.set_index(\"new_index\")\n",
    "efficiency_df = pd.DataFrame(\n",
    "    index=range(2020, 2051), \n",
    "    columns=investment_options.index, \n",
    ")\n",
    "efficiency_df.loc[2020] = investment_options[\"efficiency_el\"]\n",
    "efficiency_df = efficiency_df.ffill()\n",
    "\n",
    "_ = convert_annual_data_to_fame_time(\n",
    "    efficiency_df,\n",
    "    save=True,\n",
    "    path=f\"{path_processed_outputs}/amiris/all_scenarios/\",\n",
    "    filename=\"efficiency_el\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687bca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
